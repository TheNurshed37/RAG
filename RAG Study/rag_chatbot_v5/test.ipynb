{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71593fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nurshed/Desktop/python/venv/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-24 19:27:54.535820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Set up Google API Key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    GOOGLE_API_KEY = getpass.getpass(\"Enter your Google API key: \")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "print(\"✅ Environment setup complete\")\n",
    "\n",
    "def initialize_docling_converter():\n",
    "    \"\"\"Initialize and return the Docling DocumentConverter with minimal options for maximum speed.\"\"\"\n",
    "    doc_converter = DocumentConverter()\n",
    "    return doc_converter\n",
    "\n",
    "def validate_and_format_cv_structure(raw_text):\n",
    "    \"\"\"\n",
    "    Uses LLM to validate and format CV structure with proper headers.\n",
    "    \"\"\"\n",
    "    STANDARD_CV_HEADERS = [\n",
    "        \"Personal Information\",\n",
    "        \"Professional Summary\",\n",
    "        \"Work Experience\", \n",
    "        \"Education\",\n",
    "        \"Skills\",\n",
    "        \"Projects\",\n",
    "        \"Certifications\",\n",
    "        \"Languages\",\n",
    "        \"References\"\n",
    "    ]\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are a CV/Resume formatting expert. Your task is to analyze the extracted CV text and ensure it is properly structured with appropriate headers.\n",
    "\n",
    "    STANDARD CV HEADERS:\n",
    "    {standard_headers}\n",
    "\n",
    "    EXTRACTED CV TEXT:\n",
    "    {raw_text}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Analyze the existing structure and headers in the text\n",
    "    2. If the text is already well-structured with clear headers that match the standard, return it as-is\n",
    "    3. If headers are missing, poorly formatted, or non-standard:\n",
    "       - Reorganize the content under appropriate standard headers\n",
    "       - Preserve ALL original information\n",
    "       - Add missing headers if content exists for them\n",
    "       - Use markdown formatting with ## for headers\n",
    "    4. Ensure the structured document flows logically\n",
    "\n",
    "    Return ONLY the properly formatted CV text with appropriate headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"🔧 Validating CV structure with LLM...\")\n",
    "        \n",
    "        # Initialize LLM for structuring\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            temperature=0.1,\n",
    "            convert_system_message_to_human=True\n",
    "        )\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"raw_text\", \"standard_headers\"]\n",
    "        )\n",
    "        \n",
    "        formatted_prompt = prompt.invoke({\n",
    "            \"raw_text\": raw_text,\n",
    "            \"standard_headers\": \", \".join(STANDARD_CV_HEADERS)\n",
    "        })\n",
    "        \n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        print(\"✅ CV structure validation completed\")\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM structuring failed: {e}\")\n",
    "        print(\"🔄 Using basic header formatting as fallback...\")\n",
    "        return add_basic_headers_fallback(raw_text)\n",
    "\n",
    "def add_basic_headers_fallback(raw_text):\n",
    "    \"\"\"\n",
    "    Basic fallback method that adds simple headers without LLM.\n",
    "    \"\"\"\n",
    "    lines = raw_text.split('\\n')\n",
    "    structured_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        lower_line = line.lower()\n",
    "        if any(keyword in lower_line for keyword in ['experience', 'work', 'job', 'employment']):\n",
    "            if not line.startswith('## '):\n",
    "                structured_lines.append('## Work Experience')\n",
    "        elif any(keyword in lower_line for keyword in ['education', 'degree', 'university', 'college']):\n",
    "            if not line.startswith('## '):\n",
    "                structured_lines.append('## Education')\n",
    "        elif any(keyword in lower_line for keyword in ['skill', 'technical', 'programming']):\n",
    "            if not line.startswith('## '):\n",
    "                structured_lines.append('## Skills')\n",
    "        elif any(keyword in lower_word for keyword in ['project', 'portfolio'] for lower_word in lower_line.split()):\n",
    "            if not line.startswith('## '):\n",
    "                structured_lines.append('## Projects')\n",
    "        elif any(keyword in lower_line for keyword in ['certificat', 'license']):\n",
    "            if not line.startswith('## '):\n",
    "                structured_lines.append('## Certifications')\n",
    "        elif any(keyword in lower_line for keyword in ['name', 'address', 'phone', 'email', 'contact']):\n",
    "            if not line.startswith('## '):\n",
    "                structured_lines.append('## Personal Information')\n",
    "        \n",
    "        structured_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(structured_lines)\n",
    "\n",
    "def demonstrate_cv_processing(pdf_path):\n",
    "    \"\"\"\n",
    "    Demonstrate the CV processing pipeline up to document creation.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🎯 CV PROCESSING PIPELINE (UP TO DOCUMENT CREATION)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Docling Extraction\n",
    "    print(\"\\n1️⃣ STEP 1: DOCLING EXTRACTION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    doc_converter = initialize_docling_converter()\n",
    "    result = doc_converter.convert(pdf_path)\n",
    "    raw_text = result.document.export_to_markdown()\n",
    "    \n",
    "    print(f\"📊 Extracted {len(raw_text)} characters\")\n",
    "    print(f\"📝 Sample of extracted text:\")\n",
    "    print(\"─\" * 40)\n",
    "    print(raw_text[:800] + \"...\" if len(raw_text) > 800 else raw_text)\n",
    "    print(\"─\" * 40)\n",
    "    \n",
    "    # Step 2: LLM Structure Validation\n",
    "    print(\"\\n2️⃣ STEP 2: LLM STRUCTURE VALIDATION & FORMATTING\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    structured_text = validate_and_format_cv_structure(raw_text)\n",
    "    \n",
    "    print(f\"📊 After structuring: {len(structured_text)} characters\")\n",
    "    print(f\"📝 Structured text preview:\")\n",
    "    print(\"─\" * 40)\n",
    "    print(structured_text[:1000] + \"...\" if len(structured_text) > 1000 else structured_text)\n",
    "    print(\"─\" * 40)\n",
    "    \n",
    "    # Step 3: Document Creation\n",
    "    print(\"\\n3️⃣ STEP 3: DOCUMENT CREATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=structured_text,\n",
    "        metadata={\n",
    "            \"source\": pdf_path,\n",
    "            \"page\": 1,\n",
    "            \"processed\": \"structured\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Created LangChain Document\")\n",
    "    print(f\"📏 Document length: {len(doc.page_content)} characters\")\n",
    "    print(f\"📋 Metadata: {doc.metadata}\")\n",
    "    print(\"─\" * 40)\n",
    "    \n",
    "    # Return all intermediate results\n",
    "    return {\n",
    "        'raw_text': raw_text,\n",
    "        'structured_text': structured_text,\n",
    "        'document': doc\n",
    "    }\n",
    "\n",
    "def show_detailed_comparison(pdf_path):\n",
    "    \"\"\"\n",
    "    Show detailed before/after comparison with header analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🔄 DETAILED BEFORE vs AFTER COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Process the CV\n",
    "    results = demonstrate_cv_processing(pdf_path)\n",
    "    raw_text = results['raw_text']\n",
    "    structured_text = results['structured_text']\n",
    "    \n",
    "    # Header Analysis\n",
    "    print(\"\\n4️⃣ HEADER ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Find headers in structured text\n",
    "    headers = [line.strip() for line in structured_text.split('\\n') if line.startswith('## ')]\n",
    "    \n",
    "    print(f\"📑 Headers found in structured CV: {len(headers)}\")\n",
    "    for i, header in enumerate(headers, 1):\n",
    "        print(f\"   {i}. {header}\")\n",
    "    \n",
    "    # Content under each header\n",
    "    print(f\"\\n📊 Content distribution:\")\n",
    "    sections = structured_text.split('## ')\n",
    "    for section in sections[1:]:  # Skip first empty section\n",
    "        if '\\n' in section:\n",
    "            header = section.split('\\n')[0].strip()\n",
    "            content = '\\n'.join(section.split('\\n')[1:]).strip()\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"   🎯 {header}: {len(content)} chars → '{content_preview}'\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5267ed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 19:29:51,008 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-24 19:29:51,012 - INFO - Going to convert document batch...\n",
      "2025-10-24 19:29:51,013 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 4f2edc0f7d9bb60b38ebfecf9a2609f5\n",
      "2025-10-24 19:29:51,015 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-10-24 19:29:51,060 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-24 19:29:51,082 [RapidOCR] download_file.py:60: File exists and is valid: /home/nurshed/Desktop/python/venv/base/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-24 19:29:51,084 [RapidOCR] main.py:53: Using /home/nurshed/Desktop/python/venv/base/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.onnx\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CV through the pipeline...\n",
      "================================================================================\n",
      "🎯 CV PROCESSING PIPELINE (UP TO DOCUMENT CREATION)\n",
      "================================================================================\n",
      "\n",
      "1️⃣ STEP 1: DOCLING EXTRACTION\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-10-24 19:29:51,249 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-24 19:29:51,256 [RapidOCR] download_file.py:60: File exists and is valid: /home/nurshed/Desktop/python/venv/base/lib/python3.12/site-packages/rapidocr/models/ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-24 19:29:51,257 [RapidOCR] main.py:53: Using /home/nurshed/Desktop/python/venv/base/lib/python3.12/site-packages/rapidocr/models/ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-24 19:29:51,355 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-24 19:29:51,415 [RapidOCR] download_file.py:60: File exists and is valid: /home/nurshed/Desktop/python/venv/base/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-24 19:29:51,417 [RapidOCR] main.py:53: Using /home/nurshed/Desktop/python/venv/base/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2025-10-24 19:29:51,541 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2025-10-24 19:29:51,541 - INFO - Accelerator device: 'cpu'\n",
      "2025-10-24 19:29:52,779 - INFO - Accelerator device: 'cpu'\n",
      "2025-10-24 19:29:53,318 - INFO - Processing document resume1.pdf\n",
      "2025-10-24 19:29:55,744 - INFO - Finished converting document resume1.pdf in 4.74 sec.\n",
      "E0000 00:00:1761312595.751301   21144 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Extracted 2139 characters\n",
      "📝 Sample of extracted text:\n",
      "────────────────────────────────────────\n",
      "## IM A. SAMPLE I\n",
      "\n",
      "1234 North 55 Street Bellevue, Nebraska 68005 (402) 292-2345\n",
      "\n",
      "imasample1@xxx.com\n",
      "\n",
      "## SUMMARY OF QUALIFICATIONS\n",
      "\n",
      "Exceptionally well organized and resourceful Professional with more than six years experience and a solid academic background in accounting and financial management; excellent analytical and problem solving skills; able to handle multiple projects while producing high quality work in a fast-paced, deadline-oriented environment.\n",
      "\n",
      "## EDUCATION\n",
      "\n",
      "Bachelor of Science\n",
      "\n",
      ", Bellevue University, Bellevue, NE (In Progress)\n",
      "\n",
      "Major:  Accounting\n",
      "\n",
      "Minor:  Computer Information Systems\n",
      "\n",
      "Expected Graduation Date:  January, 20xx\n",
      "\n",
      "GPA to date:  3.95/4.00\n",
      "\n",
      "## PROFESSIONAL ACCOMPLISHMENTS\n",
      "\n",
      "## Accounting and Financial Management\n",
      "\n",
      "-  Developed and maintained accounting records for up...\n",
      "────────────────────────────────────────\n",
      "\n",
      "2️⃣ STEP 2: LLM STRUCTURE VALIDATION & FORMATTING\n",
      "--------------------------------------------------\n",
      "🔧 Validating CV structure with LLM...\n",
      "✅ CV structure validation completed\n",
      "📊 After structuring: 2079 characters\n",
      "📝 Structured text preview:\n",
      "────────────────────────────────────────\n",
      "## Personal Information\n",
      "IM A. SAMPLE I\n",
      "1234 North 55 Street Bellevue, Nebraska 68005\n",
      "(402) 292-2345\n",
      "imasample1@xxx.com\n",
      "\n",
      "## Professional Summary\n",
      "Exceptionally well organized and resourceful Professional with more than six years experience and a solid academic background in accounting and financial management; excellent analytical and problem solving skills; able to handle multiple projects while producing high quality work in a fast-paced, deadline-oriented environment.\n",
      "\n",
      "## Accounting and Financial Management\n",
      "- Developed and maintained accounting records for up to fifty bank accounts.\n",
      "- Formulated monthly and year-end financial statements and generated various payroll records, including federal and state payroll reports, annual tax reports, W-2 and 1099 forms, etc.\n",
      "- Tested accuracy of account balances and prepared supporting documentation for submission during a comprehensive three-year audit of financial operations.\n",
      "- Formulated intricate pro-forma budgets.\n",
      "- Calculated and implemente...\n",
      "────────────────────────────────────────\n",
      "\n",
      "3️⃣ STEP 3: DOCUMENT CREATION\n",
      "--------------------------------------------------\n",
      "✅ Created LangChain Document\n",
      "📏 Document length: 2079 characters\n",
      "📋 Metadata: {'source': '/home/nurshed/Desktop/python/project/RAG Study/rag_chatbot_v5/resume1.pdf', 'page': 1, 'processed': 'structured'}\n",
      "────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Process a single CV and see the transformation\n",
    "print(\"Processing CV through the pipeline...\")\n",
    "pdf_path = \"/home/nurshed/Desktop/python/project/RAG Study/rag_chatbot_v5/resume1.pdf\" \n",
    "results = demonstrate_cv_processing(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc24e34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detailed_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_doc = \u001b[43mdetailed_results\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(final_doc.page_content)\n",
      "\u001b[31mNameError\u001b[39m: name 'detailed_results' is not defined"
     ]
    }
   ],
   "source": [
    "final_doc = detailed_results['document']\n",
    "print(final_doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40abceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
